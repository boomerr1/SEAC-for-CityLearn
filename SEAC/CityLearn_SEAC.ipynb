{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn.citylearn import CityLearnEnv\n",
    "import torch\n",
    "from a2c import A2C\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "\n",
    "dataset_name = 'citylearn_challenge_2022_phase_1'\n",
    "env = CityLearnEnv(dataset_name, central_agent=False, simulation_end_time_step=1000)\n",
    "\n",
    "agents = [\n",
    "        A2C(i, osp, asp, lr=3e-4, adam_eps=0.001, recurrent_policy=False, num_steps=1000, num_processes=1, device='cpu')\n",
    "        for i, (osp, asp) in enumerate(zip(env.observation_space, env.action_space))\n",
    "    ]\n",
    "\n",
    "obs = torch.tensor(env.reset())\n",
    "for i in range(len(obs)):\n",
    "    agents[i].storage.obs[0].copy_(obs[i])\n",
    "    agents[i].storage.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'Tensor' and 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lcdew\\Desktop\\Onedrive-Folder\\CS - AI\\Seminar Advanced Deep RL\\SEAC\\CityLearn_SEAC.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lcdew/Desktop/Onedrive-Folder/CS%20-%20AI/Seminar%20Advanced%20Deep%20RL/SEAC/CityLearn_SEAC.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(n_action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lcdew/Desktop/Onedrive-Folder/CS%20-%20AI/Seminar%20Advanced%20Deep%20RL/SEAC/CityLearn_SEAC.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Obser reward and next obs\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lcdew/Desktop/Onedrive-Folder/CS%20-%20AI/Seminar%20Advanced%20Deep%20RL/SEAC/CityLearn_SEAC.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m obs, reward, done, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(n_action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lcdew/Desktop/Onedrive-Folder/CS%20-%20AI/Seminar%20Advanced%20Deep%20RL/SEAC/CityLearn_SEAC.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# envs.envs[0].render()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lcdew/Desktop/Onedrive-Folder/CS%20-%20AI/Seminar%20Advanced%20Deep%20RL/SEAC/CityLearn_SEAC.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lcdew/Desktop/Onedrive-Folder/CS%20-%20AI/Seminar%20Advanced%20Deep%20RL/SEAC/CityLearn_SEAC.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# If done then clean the history of observations.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lcdew/Desktop/Onedrive-Folder/CS%20-%20AI/Seminar%20Advanced%20Deep%20RL/SEAC/CityLearn_SEAC.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m masks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([[\u001b[39m0.0\u001b[39m] \u001b[39mif\u001b[39;00m done_ \u001b[39melse\u001b[39;00m [\u001b[39m1.0\u001b[39m] \u001b[39mfor\u001b[39;00m done_ \u001b[39min\u001b[39;00m done])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\citylearn\\citylearn.py:744\u001b[0m, in \u001b[0;36mCityLearnEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    741\u001b[0m actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_actions(actions)\n\u001b[0;32m    743\u001b[0m \u001b[39mfor\u001b[39;00m building, building_actions \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuildings, actions):\n\u001b[1;32m--> 744\u001b[0m     building\u001b[39m.\u001b[39;49mapply_actions(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuilding_actions)\n\u001b[0;32m    746\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_time_step()\n\u001b[0;32m    748\u001b[0m \u001b[39m# NOTE:\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[39m# This call to retrieve each building's observation dictionary is an expensive call especially since the observations \u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[39m# are retrieved again to send to agent but the observations in dict form is needed for the reward function to easily\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[39m# extract building-level values. Can't think of a better way to handle this without giving the reward direct access to\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[39m# env, which is not the best design for competition integrity sake. Will revisit the building.observations() function\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[39m# to see how it can be optimized.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\citylearn\\building.py:745\u001b[0m, in \u001b[0;36mBuilding.apply_actions\u001b[1;34m(self, cooling_device_action, heating_device_action, cooling_storage_action, heating_storage_action, dhw_storage_action, electrical_storage_action)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_heating(heating_device_action, heating_storage_action)\n\u001b[0;32m    744\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_dhw(dhw_storage_action)\n\u001b[1;32m--> 745\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_electrical_storage(electrical_storage_action)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\citylearn\\building.py:852\u001b[0m, in \u001b[0;36mBuilding.update_electrical_storage\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Charge/discharge `electrical_storage`.\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \n\u001b[0;32m    845\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[39m    Fraction of `electrical_storage` `capacity` to charge/discharge by.\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    851\u001b[0m energy \u001b[39m=\u001b[39m action\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39melectrical_storage\u001b[39m.\u001b[39mcapacity\n\u001b[1;32m--> 852\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49melectrical_storage\u001b[39m.\u001b[39;49mcharge(energy)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\citylearn\\energy_model.py:852\u001b[0m, in \u001b[0;36mBattery.charge\u001b[1;34m(self, energy)\u001b[0m\n\u001b[0;32m    849\u001b[0m     energy_limit_wrt_dod \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(soc_difference\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapacity\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mefficiency, \u001b[39m0.0\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    850\u001b[0m     energy \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_max_output_power(), energy_limit_wrt_dod, energy)\n\u001b[1;32m--> 852\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mefficiency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_current_efficiency(energy)\n\u001b[0;32m    853\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcharge(energy)\n\u001b[0;32m    854\u001b[0m degraded_capacity \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegraded_capacity \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegrade(), \u001b[39m0.0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\citylearn\\energy_model.py:904\u001b[0m, in \u001b[0;36mBattery.get_current_efficiency\u001b[1;34m(self, energy)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpower_efficiency_curve \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    902\u001b[0m     \u001b[39m# Calculating the maximum power rate at which the battery can be charged or discharged\u001b[39;00m\n\u001b[0;32m    903\u001b[0m     energy_normalized \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(energy)\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnominal_power\n\u001b[1;32m--> 904\u001b[0m     idx \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, np\u001b[39m.\u001b[39margmax(energy_normalized \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpower_efficiency_curve[\u001b[39m0\u001b[39;49m]) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    905\u001b[0m     efficiency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpower_efficiency_curve[\u001b[39m1\u001b[39m][idx]\\\n\u001b[0;32m    906\u001b[0m         \u001b[39m+\u001b[39m (energy_normalized \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpower_efficiency_curve[\u001b[39m0\u001b[39m][idx]\n\u001b[0;32m    907\u001b[0m         )\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpower_efficiency_curve[\u001b[39m1\u001b[39m][idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpower_efficiency_curve[\u001b[39m1\u001b[39m][idx]\n\u001b[0;32m    908\u001b[0m         )\u001b[39m/\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpower_efficiency_curve[\u001b[39m0\u001b[39m][idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpower_efficiency_curve[\u001b[39m0\u001b[39m][idx])\n\u001b[0;32m    909\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'Tensor' and 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "num_updates = (\n",
    "    5\n",
    ")\n",
    "\n",
    "all_infos = deque(maxlen=10)\n",
    "\n",
    "for j in range(1, num_updates + 1):\n",
    "\n",
    "    for step in range(1000):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            n_value, n_action, n_action_log_prob, n_recurrent_hidden_states = zip(\n",
    "                *[\n",
    "                    agent.model.act(\n",
    "                        agent.storage.obs[step],\n",
    "                        agent.storage.recurrent_hidden_states[step],\n",
    "                        agent.storage.masks[step],\n",
    "                    )\n",
    "                    for agent in agents\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        print(n_action)\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = env.step(n_action)\n",
    "        # envs.envs[0].render()\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "\n",
    "        bad_masks = torch.FloatTensor(\n",
    "            [\n",
    "                [0.0] if info.get(\"TimeLimit.truncated\", False) else [1.0]\n",
    "                for info in infos\n",
    "            ]\n",
    "        )\n",
    "        for i in range(len(agents)):\n",
    "            agents[i].storage.insert(\n",
    "                obs[i],\n",
    "                n_recurrent_hidden_states[i],\n",
    "                n_action[i],\n",
    "                n_action_log_prob[i],\n",
    "                n_value[i],\n",
    "                reward[:, i].unsqueeze(1),\n",
    "                masks,\n",
    "                bad_masks,\n",
    "            )\n",
    "\n",
    "        for info in infos:\n",
    "            if info:\n",
    "                all_infos.append(info)\n",
    "\n",
    "    # value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "    for agent in agents:\n",
    "        agent.compute_returns()\n",
    "\n",
    "    for agent in agents:\n",
    "        loss = agent.update([a.storage for a in agents])\n",
    "        for k, v in loss.items():\n",
    "            print(f\"agent{agent.agent_id}/{k}\", v, j)\n",
    "\n",
    "    for agent in agents:\n",
    "        agent.storage.after_update()\n",
    "\n",
    "    # if save_interval is not None and (\n",
    "    #     j > 0 and j % save_interval == 0 or j == num_updates\n",
    "    # ):\n",
    "    #     cur_save_dir = path.join(save_dir, f\"u{j}\")\n",
    "    #     for agent in agents:\n",
    "    #         save_at = path.join(cur_save_dir, f\"agent{agent.agent_id}\")\n",
    "    #         os.makedirs(save_at, exist_ok=True)\n",
    "    #         agent.save(save_at)\n",
    "    #     archive_name = shutil.make_archive(cur_save_dir, \"xztar\", save_dir, f\"u{j}\")\n",
    "    #     shutil.rmtree(cur_save_dir)\n",
    "\n",
    "    # if eval_interval is not None and (\n",
    "    #     j > 0 and j % eval_interval == 0 or j == num_updates\n",
    "    # ):\n",
    "    #     evaluate(\n",
    "    #         agents, os.path.join(eval_dir, f\"u{j}\"),\n",
    "    #     )\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python CityLearn",
   "language": "python",
   "name": "sadrl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
